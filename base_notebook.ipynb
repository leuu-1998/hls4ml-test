{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4aa6fb3-23c8-4af7-afb3-cb99255f1b99",
   "metadata": {},
   "source": [
    "## 1 Create and Train the AlexNet Model in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7513b-5fdb-4801-89ee-a0a43f34d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Create a CNN model for MNIST with explicit layer naming\n",
    "# This is crucial for HLS4ML compatibility\n",
    "model = Sequential([\n",
    "    Conv2D(8, kernel_size=(3, 3), padding='valid', activation='relu', \n",
    "           input_shape=(28, 28, 1), name='conv2d_1'),\n",
    "    MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_1'),\n",
    "    Conv2D(16, kernel_size=(3, 3), padding='valid', activation='relu', name='conv2d_2'),\n",
    "    MaxPooling2D(pool_size=(2, 2), name='max_pooling2d_2'),\n",
    "    Flatten(name='flatten'),\n",
    "    Dense(32, activation='relu', name='dense_1'),\n",
    "    Dense(10, activation='softmax', name='dense_2')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Add learning rate reduction to improve training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=128, \n",
    "    epochs=15,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {score[1]*100:.2f}%\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the model in HDF5 format\n",
    "model.save('mnist_cnn.h5')\n",
    "print(\"Model saved as mnist_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd185e-93cc-4373-820f-fe82aec0be1c",
   "metadata": {},
   "source": [
    "## TEST THE MODEL IN PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8a45e-5291-400b-9678-0dc01b095060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code after training the model in Step 2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_digit_from_file(model, image_path):\n",
    "    \"\"\"\n",
    "    Load an image from file and predict the digit using PIL\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image and convert to grayscale\n",
    "        img = Image.open(image_path).convert('L')\n",
    "        \n",
    "        # Resize to 28x28\n",
    "        img = img.resize((28, 28))\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Invert if needed (MNIST has white digits on black background)\n",
    "        # Uncomment if your image has black digits on white background\n",
    "        # img_array = 255 - img_array\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        img_array = img_array.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape for model input\n",
    "        img_input = img_array.reshape(1, 28, 28, 1)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_input)\n",
    "        digit = np.argmax(prediction)\n",
    "        confidence = np.max(prediction) * 100\n",
    "        \n",
    "        return digit, confidence, img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Path to your test image\n",
    "#test_image_path = 'data/test/4_image.png'  # Replace with your image path\n",
    "test_image_path = 'data/test/9_image.jpg'\n",
    "\n",
    "# Predict the digit\n",
    "digit, confidence, processed_img = predict_digit_from_file(model, test_image_path)\n",
    "\n",
    "if digit is not None:\n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    original_img = Image.open(test_image_path)\n",
    "    plt.imshow(original_img, cmap='gray' if original_img.mode == 'L' else None)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Processed image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(processed_img, cmap='gray')\n",
    "    plt.title(f'Processed Image\\nPrediction: {digit} (Confidence: {confidence:.2f}%)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Predicted digit: {digit}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7819ad-d9e9-42d2-a592-3cbe2f480859",
   "metadata": {},
   "source": [
    "## 2 Convert TensorFlow Model to HLS using HLS4ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18ffd5-11d6-4806-b82e-78a390695bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hls4ml==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffea8dc-6858-4bbf-bd3a-ae0c4feba85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"HLS4ML version: {hls4ml.__version__}\")\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('mnist_cnn.h5')\n",
    "\n",
    "# Print model summary for reference\n",
    "model.summary()\n",
    "\n",
    "# Print layer names to verify explicit naming\n",
    "print(\"Layer names in the model:\")\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i}: {layer.name} ({layer.__class__.__name__})\")\n",
    "\n",
    "# Create configuration for Vitis backend\n",
    "print(\"Creating HLS4ML configuration...\")\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "# Set precision and other parameters\n",
    "config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "config['Model']['ReuseFactor'] = 3  # Using 3 instead of 4 based on the warning\n",
    "config['Model']['Backend'] = 'Vitis'\n",
    "config['Model']['SerializeOutput'] = True\n",
    "config['Model']['Strategy'] = 'Resource'\n",
    "\n",
    "# Set specific configurations for Conv2D layers\n",
    "# Using direct assignment to ensure the correct layer names are used\n",
    "config['conv2d_1'] = {'Strategy': 'Resource', 'ConvImplementation': 'LineBuffer', 'ReuseFactor': 3}\n",
    "config['conv2d_2'] = {'Strategy': 'Resource', 'ConvImplementation': 'LineBuffer', 'ReuseFactor': 3}\n",
    "\n",
    "# Print configuration for verification\n",
    "print(\"HLS4ML Configuration:\")\n",
    "for key in config.keys():\n",
    "    if key == 'Model':\n",
    "        print(f\"Model settings: {config['Model']}\")\n",
    "    else:\n",
    "        print(f\"Layer '{key}' settings: {config[key]}\")\n",
    "\n",
    "# Convert model to HLS\n",
    "print(\"Converting model to HLS with Vitis backend...\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=config,\n",
    "    output_dir='hls4ml_mnist_vitis_prj',\n",
    "    part='xc7z020clg400-1',\n",
    "    backend='Vitis'\n",
    ")\n",
    "\n",
    "# Write HLS files\n",
    "print(\"Generating HLS files for Vitis...\")\n",
    "hls_model.write()\n",
    "\n",
    "print(\"HLS files generated successfully in 'hls4ml_mnist_vitis_prj' directory\")\n",
    "print(\"You can now use these files to create an IP core in Vitis HLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd885a8-d700-4171-83d6-c5e19aa0e25d",
   "metadata": {},
   "source": [
    "## 3 Create a Vivado Project and Integrate the HLS IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1ad6b-a181-43b5-85ee-319833816119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash code\n",
    "# These are shell commands to be executed in a terminal\n",
    "\n",
    "# 1. Create a new Vivado project for Zynq-7000\n",
    "vivado -mode batch -source create_project.tcl\n",
    "\n",
    "# 2. Import the HLS IP into the Vivado project\n",
    "# 3. Connect the IP to the Zynq PS through AXI interfaces\n",
    "# 4. Generate the bitstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d27ff-7a8f-4b5e-a935-302fb7e60ede",
   "metadata": {},
   "source": [
    "The create_project.tcl file should contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f23d4-e98b-4f98-98ec-b8f861f039d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tcl code\n",
    "# Create Vivado project\n",
    "create_project alexnet_mnist_zynq ./alexnet_mnist_zynq -part xc7z020clg400-1\n",
    "set_property board_part www.digilentinc.com:pynq-z1:part0:1.0 [current_project]\n",
    "\n",
    "# Create block design\n",
    "create_bd_design \"design_1\"\n",
    "update_compile_order -fileset sources_1\n",
    "\n",
    "# Add Zynq PS\n",
    "create_bd_cell -type ip -vlnv xilinx.com:ip:processing_system7:5.5 processing_system7_0\n",
    "apply_bd_automation -rule xilinx.com:bd_rule:processing_system7 -config {make_external \"FIXED_IO, DDR\" apply_board_preset \"1\" Master \"Disable\" Slave \"Disable\" }  [get_bd_cells processing_system7_0]\n",
    "\n",
    "# Add HLS IP\n",
    "set_property  ip_repo_paths  {./hls4ml_prj/alexnet_mnist_prj/solution1/impl/ip} [current_project]\n",
    "update_ip_catalog\n",
    "create_bd_cell -type ip -vlnv xilinx.com:hls:alexnet_mnist:1.0 alexnet_mnist_0\n",
    "\n",
    "# Connect AXI interfaces\n",
    "apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {Auto} Clk_slave {Auto} Clk_xbar {Auto} Master {/processing_system7_0/M_AXI_GP0} Slave {/alexnet_mnist_0/s_axi_control} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins alexnet_mnist_0/s_axi_control]\n",
    "\n",
    "# Create wrapper\n",
    "make_wrapper -files [get_files ./alexnet_mnist_zynq/alexnet_mnist_zynq.srcs/sources_1/bd/design_1/design_1.bd] -top\n",
    "add_files -norecurse ./alexnet_mnist_zynq/alexnet_mnist_zynq.gen/sources_1/bd/design_1/hdl/design_1_wrapper.v\n",
    "update_compile_order -fileset sources_1\n",
    "\n",
    "# Generate bitstream\n",
    "launch_runs impl_1 -to_step write_bitstream -jobs 4\n",
    "wait_on_run impl_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930d916-3df3-46b1-a3d1-d441a00e4250",
   "metadata": {},
   "source": [
    "## 4 Develop PYNQ Python Code for FPGA Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415b30e-c109-471e-abd6-ba904e3a3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs on the PYNQ board\n",
    "from pynq import Overlay\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Load the overlay\n",
    "overlay = Overlay('/home/xilinx/alexnet_mnist.bit')\n",
    "\n",
    "# Access the HLS IP\n",
    "alexnet_ip = overlay.alexnet_mnist_0\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    # Load image and convert to grayscale\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    # Resize to 28x28\n",
    "    img = img.resize((28, 28))\n",
    "    # Convert to numpy array and normalize\n",
    "    img_array = np.array(img).astype('float32') / 255.0\n",
    "    # Reshape for the model\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)\n",
    "    # Flatten for the HLS IP\n",
    "    return img_array.flatten()\n",
    "\n",
    "# Function to run inference\n",
    "def recognize_number(image_path):\n",
    "    # Preprocess the image\n",
    "    input_data = preprocess_image(image_path)\n",
    "    \n",
    "    # Write input data to the IP\n",
    "    for i, val in enumerate(input_data):\n",
    "        alexnet_ip.write(i*4, val)\n",
    "    \n",
    "    # Start the IP\n",
    "    alexnet_ip.write(0x00, 1)\n",
    "    \n",
    "    # Wait for completion\n",
    "    while alexnet_ip.read(0x00) & 0x4 == 0:\n",
    "        pass\n",
    "    \n",
    "    # Read the results\n",
    "    result = [alexnet_ip.read(0x100 + i*4) for i in range(10)]\n",
    "    \n",
    "    # Return the predicted digit\n",
    "    return np.argmax(result)\n",
    "\n",
    "# Example usage\n",
    "image_path = 'test_digit.png'\n",
    "start_time = time.time()\n",
    "predicted_digit = recognize_number(image_path)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Predicted digit: {predicted_digit}\")\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238a335-799a-48ba-99f3-589399720fce",
   "metadata": {},
   "source": [
    "## 5 Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104993c0-bb04-4d96-802c-066921bdbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs on the PYNQ board\n",
    "from pynq import Overlay\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Load test images from MNIST test set\n",
    "# Assuming you've transferred some test images to the PYNQ board\n",
    "test_images = ['test_digit_0.png', 'test_digit_1.png', 'test_digit_2.png', \n",
    "               'test_digit_3.png', 'test_digit_4.png']\n",
    "expected_labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total_time = 0\n",
    "\n",
    "for img_path, label in zip(test_images, expected_labels):\n",
    "    start_time = time.time()\n",
    "    prediction = recognize_number(img_path)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    total_time += inference_time\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f\"Image: {img_path}, Expected: {label}, Predicted: {prediction}, Time: {inference_time:.4f}s\")\n",
    "\n",
    "accuracy = correct / len(test_images) * 100\n",
    "avg_time = total_time / len(test_images)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Average inference time: {avg_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef30424-238b-4e1d-bdad-cb8346ab76ad",
   "metadata": {},
   "source": [
    "## 6 Performance Optimization (at the end of all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0cf7c-b200-4316-ba00-f9607b6cbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs on the PYNQ board\n",
    "# Measure and optimize performance\n",
    "\n",
    "# Measure power consumption\n",
    "from pynq import Overlay, MMIO\n",
    "import time\n",
    "\n",
    "# Function to measure power\n",
    "def measure_power(duration=5):\n",
    "    # Access power management registers\n",
    "    power_rails = ['VCC_INT', 'VCC_AUX', 'VCC_BRAM']\n",
    "    power_values = {}\n",
    "    \n",
    "    for rail in power_rails:\n",
    "        power = 0\n",
    "        samples = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Read power values (implementation depends on specific board)\n",
    "            # This is a simplified example\n",
    "            power += 0.1  # Replace with actual power reading\n",
    "            samples += 1\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        power_values[rail] = power / samples\n",
    "    \n",
    "    return power_values\n",
    "\n",
    "# Measure baseline power\n",
    "print(\"Measuring baseline power...\")\n",
    "baseline_power = measure_power()\n",
    "\n",
    "# Run inference and measure power\n",
    "print(\"Measuring inference power...\")\n",
    "overlay = Overlay('/home/xilinx/alexnet_mnist.bit')\n",
    "alexnet_ip = overlay.alexnet_mnist_0\n",
    "\n",
    "# Run inference multiple times\n",
    "for _ in range(10):\n",
    "    recognize_number('test_digit.png')\n",
    "\n",
    "inference_power = measure_power()\n",
    "\n",
    "# Calculate power difference\n",
    "for rail in baseline_power:\n",
    "    power_diff = inference_power[rail] - baseline_power[rail]\n",
    "    print(f\"{rail} power increase during inference: {power_diff:.4f} W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd154a9b-7802-47c8-a84a-e07a6206ab70",
   "metadata": {},
   "source": [
    "## 7 Web based service with the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e04ad-9206-4184-8dbe-0ef02ff32753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs on the PYNQ board\n",
    "# Create a simple web interface for digit recognition\n",
    "\n",
    "from pynq import Overlay\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "from flask import Flask, request, jsonify, render_template_string\n",
    "\n",
    "# Load the overlay\n",
    "overlay = Overlay('/home/xilinx/alexnet_mnist.bit')\n",
    "alexnet_ip = overlay.alexnet_mnist_0\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# HTML template for the web interface\n",
    "HTML_TEMPLATE = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>FPGA Digit Recognition</title>\n",
    "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
    "    <style>\n",
    "        canvas {\n",
    "            border: 1px solid black;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Draw a digit (0-9)</h1>\n",
    "    <canvas id=\"canvas\" width=\"280\" height=\"280\"></canvas>\n",
    "    <div>\n",
    "        <button id=\"clear\">Clear</button>\n",
    "        <button id=\"recognize\">Recognize</button>\n",
    "    </div>\n",
    "    <h2>Result: <span id=\"result\"></span></h2>\n",
    "    \n",
    "    <script>\n",
    "        const canvas = document.getElementById('canvas');\n",
    "        const ctx = canvas.getContext('2d');\n",
    "        let isDrawing = false;\n",
    "        \n",
    "        ctx.lineWidth = 15;\n",
    "        ctx.lineCap = 'round';\n",
    "        ctx.lineJoin = 'round';\n",
    "        ctx.strokeStyle = 'black';\n",
    "        \n",
    "        canvas.addEventListener('mousedown', startDrawing);\n",
    "        canvas.addEventListener('mousemove', draw);\n",
    "        canvas.addEventListener('mouseup', stopDrawing);\n",
    "        canvas.addEventListener('mouseout', stopDrawing);\n",
    "        \n",
    "        document.getElementById('clear').addEventListener('click', clearCanvas);\n",
    "        document.getElementById('recognize').addEventListener('click', recognizeDigit);\n",
    "        \n",
    "        function startDrawing(e) {\n",
    "            isDrawing = true;\n",
    "            draw(e);\n",
    "        }\n",
    "        \n",
    "        function draw(e) {\n",
    "            if (!isDrawing) return;\n",
    "            \n",
    "            ctx.beginPath();\n",
    "            ctx.moveTo(e.offsetX, e.offsetY);\n",
    "            ctx.lineTo(e.offsetX, e.offsetY);\n",
    "            ctx.stroke();\n",
    "        }\n",
    "        \n",
    "        function stopDrawing() {\n",
    "            isDrawing = false;\n",
    "        }\n",
    "        \n",
    "        function clearCanvas() {\n",
    "            ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
    "            document.getElementById('result').textContent = '';\n",
    "        }\n",
    "        \n",
    "        function recognizeDigit() {\n",
    "            const imageData = canvas.toDataURL('image/png');\n",
    "            \n",
    "            $.ajax({\n",
    "                url: '/recognize',\n",
    "                type: 'POST',\n",
    "                contentType: 'application/json',\n",
    "                JSON.stringify({image: imageData}),\n",
    "                success: function(response) {\n",
    "                    document.getElementById('result').textContent = response.digit;\n",
    "                }\n",
    "            });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string(HTML_TEMPLATE)\n",
    "\n",
    "@app.route('/recognize', methods=['POST'])\n",
    "def recognize():\n",
    "    # Get the image data from the request\n",
    "    image_data = request.json['image']\n",
    "    image_data = image_data.split(',')[1]\n",
    "    \n",
    "    # Decode the base64 image\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(image_data)))\n",
    "    \n",
    "    # Convert to grayscale and resize\n",
    "    image = image.convert('L').resize((28, 28))\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    image_array = np.array(image).astype('float32') / 255.0\n",
    "    \n",
    "    # Invert colors (MNIST has white digits on black background)\n",
    "    image_array = 1 - image_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
